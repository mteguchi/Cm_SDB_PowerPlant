---
title: "R Notebook"
output: html_notebook
---

San Diego Bay Turtle Movement Analysis
Original R Code, JT Froeschke, December 29, 2015
Data modified from previous versions by SE Graham, June 2016, May 2018
Data modified from May 2018 by T EGuchi, July 2018

Filtered data utilize GPS, and Argos LC = 1,2,3
Filtered data do not include points on land
Filtered data only allow for 1 relocation every 4 horus

Purpose of the script is to compute homerange (area) using 
least squares cross-validation including 50% and 95% contours.
An analysis of each turtle and an aggregate pre and post will be computed
h values are chosen based on best judgment and gst behavior  

```{r}
rm(list = ls())
#getwd()
#list.files()

#Section 1: Load libraries and set wd
#library(readxl)
library(dplyr)
library(adehabitatHR)
library(rgdal)
library(leaflet) 
library(ggplot2)
library(lubridate)
library(tidyverse)

ifelse(Sys.info()[1] == 'Linux',
       source('~/Documents/R/tools/TomosFunctions.R'),
       source('~/R/tools/TomosFunctions.R'))

internet <- F #TRUE
save.figure <- F

# Minimum number of relocations per individual to be included in HR analysis
min_n <- 50

# a function to create a data frame with minimum number of data points per individual
# then convert geographic coordinates so HR can be computed. Creates a list of length
# four. tagproj is the projection definition, which is defined at the end of next chunk

make.HR.dataset <- function(data.all, ID.min_n, tagproj){
  selected <- right_join(data.all, 
                         ID.min_n, 
                         by = "ArgosID")
  
  all.coords <- data.frame(x=selected$Lon1, 
                           y=selected$Lat1)
  
  coordinates(all.coords) <- ~ x + y
  proj4string(all.coords) <- CRS(latlong)
  all.utm <- spTransform(all.coords, tagproj)

  byID.coords <- data.frame(x=selected$Lon1, 
                            y=selected$Lat1,
                            ID = as.factor(selected$ArgosID))
  coordinates(byID.coords) <- ~ x + y
  proj4string(byID.coords) <- CRS(latlong)
  byID.utm <- spTransform(byID.coords, tagproj)
  
  eachID.utm <- eachID.coords <- list()
  
  unique.ID <- unique(selected$ArgosID)
  for (k in 1:length(unique.ID)){
    tmp.data <- filter(selected, ArgosID == unique.ID[k])
    tmp1 <- data.frame(x = tmp.data$Lon1,
                       y = tmp.data$Lat1)
    
    coordinates(tmp1) <- ~ x + y
    proj4string(tmp1) <- CRS(latlong)
    tmp2 <- spTransform(tmp1, tagproj)
    
    eachID.coords[[k]] <- tmp1
    eachID.utm[[k]] <- tmp2
  }
  
  return(list(all.coords = all.coords,
              all.utm = all.utm,
              byID.coords = byID.coords,
              byID.utm = byID.utm,
              eachID.coords = eachID.coords,
              eachID.utm = eachID.utm,
              unique.ID = unique.ID))
}

# A function to compute UDs using all data and for each individual. Returns a list 
# of estimated bandwidth, kernel density estimates as well as areas
# for 50 and 95% UDs
HR.bvnorm.fcn <- function(all.utm, 
                          byID.utm, 
                          h="href", 
                          hlim=c(0.03, 1.5), 
                          grid=300, extent = 1){
  
  all.kd <- kernelUD(all.utm, 
                     h = h,  
                     hlim = hlim, 
                     grid = grid,
                     extent = extent,
                     kern = "bivnorm")
  
  Area.all <- kernel.area(all.kd, 
                          percent = c(50, 95),
                          unin = c("m"),
                          unout = c("km2"), 
                          standardize = FALSE)

  byID.kd <- kernelUD(byID.utm, 
                      h = all.kd@h$h ,  
                      hlim = hlim, 
                      grid = grid,
                      extent = extent,
                      kern = "bivnorm")
  
  Area.byID <- kernel.area(byID.kd, 
                           percent = c(50, 95),
                           unin = c("m"),
                           unout = c("km2"), 
                           standardize = FALSE)

  bw <- all.kd@h$h ##bandwidth estimate
  #Area.50 <- Area.all["50"]
  #Area.95 <- Area.all["95"]
  
  #ver.50 <- getverticeshr(all.kd, 50)
  #ver.95 <- getverticeshr(all.kd, 95)
  
  return(list(bw = bw,
              all.kd = all.kd,
              byID.kd = byID.kd,
              area.all = Area.all,
              area.byID = Area.byID))
}

# A function to conduct the first step of UD analysis. It takes a full dataset
# and select individuals with at least min_n (50) data points. Computes UD using the 
# reference bandwidth (href) and the method of least square cross validation (LSCV).
# UD from the LSCV method can be plotted to find the bandwidth that minimizes the
# CV of bandwidth. 
HR.analysis.step1 <- function(min_n = 50, data.df, tagproj, grid=300){
  data.df %>% count(by = ArgosID) %>%
    filter(n > (min_n - 1)) %>%
    #dplyr::select(by) %>%
    rename(ArgosID = by) -> ID.min_n_day
  
  list.data <- make.HR.dataset(data.df, ID.min_n_day, tagproj)

  kd.href <- kernelUD(list.data$all.utm, 
                      h="href",  
                      grid=grid,
                      kern = "bivnorm")
  
  kd.LSCV <- kernelUD(list.data$all.utm, 
                      h="LSCV",  
                      hlim = c(0.03, 1.5), 
                      grid=300,
                      kern = "bivnorm")
  return(list(kd.href = kd.href,
              kd.LSCV = kd.LSCV,
              IDs = ID.min_n_day,
              list.data = list.data))
}

# A function to conduct home range analysis for a wide range of bandwidth values.
# INputs are multiplier values for the reference bandwidth, e.g., seq(from = 0.05, 
# to = 0.95, by = 0.05), estimated reference bandwidth (found in the output of
# HR.analysis.step1), an output of make.HR.dataset, and grid size (default is 300)
# It returns a figure (faceted 95% UDs in SDB), a dataframe used to make the figure,
# and bandwidth values that correspond to mulipliers. 
HR.analysis <- function(h.multiplier, kd.href, data.list, grid = 300){
  h.adhoc <- h.multiplier * kd.href@h$h

  kd.adhoc <- vector(mode = "list", length = length(h.adhoc))
  for (k in 1:length(h.adhoc)){
    kd.adhoc[[k]] <- kernelUD(data.list$all.utm, 
                              h = h.adhoc[k],  
                              grid = grid,
                              kern = "bivnorm")
    
  }

  # make dataframes of vertices:
  ver.95.adhoc <- lapply(kd.adhoc, 
                         FUN = getverticeshr, 
                         percent = 95)

  ver.95.adhoc.tmp <- lapply(ver.95.adhoc, 
                             FUN = spTransform, 
                             CRS = CRS("+proj=longlat"))

  ver.95.adhoc.list <- lapply(ver.95.adhoc.tmp,
                              FUN = broom::tidy)

  for (k in 1:length(ver.95.adhoc.list)){
    ver.95.adhoc.list[[k]] <- ver.95.adhoc.list[[k]] %>% 
      mutate(h = h.adhoc[k]) %>%
      mutate(h.fac = as.factor(round(h, digits = 2))) %>%
      mutate(h.multip = h.multiplier[k])
  }

  # rbind all:
  ver.95.adhoc.df <- do.call("rbind", ver.95.adhoc.list)

  p.h.adhoc <- ggplot(data = ver.95.adhoc.df,
                      aes(x = long, y = lat,
                          group = group)) + 
    geom_polygon() + 
    coord_map() + facet_grid(. ~ h.multip) +
    facet_wrap( ~ h.multip, nrow = 3) + 
    labs(x = "", y = "")+ 
    theme(axis.text.x = element_text(angle = 90)) +
    scale_x_continuous(breaks = c(-117.14, -117.12, -117.10),
                       limits = c(-117.15, -117.09))+
    scale_y_continuous(breaks = c(32.60, 32.62, 32.64),
                       limits = c(32.60, 32.66))

  return(list(figure = p.h.adhoc,
              ver.95 = ver.95.adhoc.df,
              h = h.adhoc))
}

# A function to compute 50 and 95% UD areas. INputs are bandwidth (h), 
# the output from make.HR.dataset, and grid value. It uses HR.bvnorm.fcn.
# Returns 50 and 95% areas and vertices, which are used to plot UDs. 
compute.area <- function(h, list.data, grid=300){
  HR <- HR.bvnorm.fcn(list.data$all.utm, 
                      list.data$byID.utm, 
                      h = h, 
                      hlim=c(0.03, 1.5), 
                      grid=grid, extent = 1)
  bw <- HR$all.kd@h$h ##bandwidth estimate

  return(list(area.50 = HR$area.all["50"],
              area.95 = HR$area.all["95"],
              ver.50 = getverticeshr(HR$all.kd, 50),
              ver.95 = getverticeshr(HR$all.kd, 95),
              area.byID.50 = HR$area.byID["50",],
              area.byID.95 = HR$area.byID["95",],
              ver.byID.50 = getverticeshr(HR$byID.kd, 50),
              ver.byID.95 = getverticeshr(HR$byID.kd, 95)))
}

# This funciton finds a bandwidth value that makes 95% UD contiguous
find.h.adhoc <- function(utm.data, grid = 300, hlim = c(0.03, 1.5), extent = 1){
  tmp.href <- kernelUD(utm.data, h = "href", kern = "bivnorm", grid = grid)
  h1 <- tmp.href@h$h
  h.multip <- 0.05
  n.seg <- 2
  k <- 1
  while (n.seg > 1){
    h <- h1 * h.multip
    tmp.UD <- kernelUD(utm.data, h = h, 
                       kern = "bivnorm", grid = grid)
    tmp.V <- getverticeshr(tmp.UD, 95)
    n.seg <- length(tmp.V@polygons[[1]]@Polygons)
    h.multip <- h.multip + 0.05
  }
  return(list(v.95 = tmp.V,
              UD.95 = tmp.UD,
              h = h,
              href = h1,
              h.multip = h.multip))
}


```

Get some base layer maps here:

```{r}
# just use the 2014 eelgrass data - there are old data files also
SDBay.eelg.2014 <- spTransform(readOGR(dsn = "GISfiles/Features",
                                       layer = "SD_Baywide_Eelgrass_2014_Final",
                                       verbose = F),
                               CRS("+proj=longlat +datum=WGS84"))
SDBay.eelg.2014.df <- broom::tidy(SDBay.eelg.2014)

SDBay.eelg.2008 <- spTransform(readOGR(dsn = "GISfiles/Features",
                                       layer = "SD_Baywide_Eelgrass_2008",
                                       verbose = F),
                               CRS("+proj=longlat +datum=WGS84"))
SDBay.eelg.2008.df <- broom::tidy(SDBay.eelg.2008)

if (internet){
  sdbay.all <- ggmap::get_map(location = c(lon = -117.15,
                                           lat = 32.65),
                       zoom = 12,
                       maptype = "satellite",
                       color = "bw",
                       filename = 'sdbay_all',
                       force = F)
  saveRDS(sdbay.all, file = 'RData/sdbay_all.rds')

  sdbay.south <- ggmap::get_map(location = c(lon = -117.117,
                                             lat = 32.625),
                         zoom = 14,
                         maptype = "satellite",
                         color = "bw",
                         filename = 'sdbay_south',
                         force = F)
  saveRDS(sdbay.south, file = 'RData/sdbay_south.rds')

  sdbay.med <- ggmap::get_map(location = c(lon =  -117.117,
                                           lat = 32.625),
                       zoom = 13,
                       maptype = "satellite",
                       color = "bw",
                       filename = 'sdbay_med',
                       force = F)
  saveRDS(sdbay.med, file = 'RData/sdbay_med.rds')
} else {
  sdbay.all <- readRDS(file = 'RData/sdbay_all.rds')
  sdbay.south <- readRDS(file = 'RData/sdbay_south.rds')
  sdbay.med <- readRDS(file = 'RData/sdbay_med.rds')
}

map.sdbay.zm <- ggmap::ggmap(sdbay.all)

map.sdbay.south <- ggmap::ggmap(sdbay.south)

map.sdbay.med <- ggmap::ggmap(sdbay.med)

# read in the SDB shape file:
SDB.shape <- readOGR(dsn = "GISFiles", layer = "sd_bay")
SDBay <- spTransform(readOGR(dsn = "GISfiles",
                             layer = "sd_bay",
                             verbose = F),
                     CRS("+proj=longlat +datum=WGS84"))
SDBay.df <- broom::tidy(SDBay)

water <- spTransform(readOGR(dsn = "GISFiles", 
                             layer = "water",
                             verbose = F),
                     CRS("+proj=longlat +datum=WGS84"))
water.df <- broom::tidy(water) %>%
  filter(lat < 32.75 & lat > 32.58 & long > -117.25)

tagprj <- readOGR(dsn = "Tag_065_UTMzone11n", 
                  layer = "tag_065_project")
tagproj <- proj4string(tagprj)

# not sure what this part is doing... 
latlong = "+init=epsg:4326"
```

Section 2: read in data

Section 2.1: Pre

```{r}
file.date <- "2018-07-26"
ID.names <- function(name){
  x <- unlist(strsplit(name, '_'))[1]
  return(x)
}
dname <- "data/files_Jul2018_withNewTags/pre/"
pre.files <- dir(path = dname, 
                 pattern = "_inout_DayNight_4hrs_2018-07-26.csv")

pre.IDs <- unlist(lapply(pre.files, FUN = ID.names))

col_def <- cols(ID = col_integer(),
                ArgosID = col_integer(),
                Message_Type = col_factor(levels = c("DIAG", "DS", "GPS")),
                TransmissionDateTime = col_datetime(format = "%m-%d-%Y %H:%M:%S"),
                LC = col_integer(),
                Residual = col_double(),
                Lat1 = col_double(),
                Lon1 = col_double(),
                inside = col_integer(),
                UTCDateTime = col_datetime(format = "%Y-%m-%d %H:%M:%S"),
                LocalDateTime = col_datetime(format = "%Y-%m-%d %H:%M:%S"),
                Date2 = col_date(format = "%Y/%m/%d"),
                LocalSunrise = col_datetime(format = "%Y-%m-%d %H:%M:%S"),
                LocalSunset =  col_datetime(format = "%Y-%m-%d %H:%M:%S"),
                day1night2 = col_integer(),
                row = col_integer(),
                include = col_integer(),
                hr = col_integer())

pre.raw <- do.call(rbind, 
                   lapply(pre.files,
                          FUN = function(x) readr::read_csv(file = paste0(dname, x),
                                                            col_types = col_def))) %>%
  mutate(ID.f = as.factor(ArgosID)) 

pre.all <- pre.raw %>%
  filter(include == 1) %>%
  group_by(ID.f) %>%
  mutate(DaysSinceFirst = as.double((LocalDateTime - min(LocalDateTime))/(60*60*24)))

# This one contains all data.
pre.all.01 <- do.call(rbind, 
                      lapply(pre.files,
                             FUN = function(x) readr::read_csv(file = paste0(dname, x),
                                                               col_types = col_def))) %>%
  mutate(ID.f = as.factor(ArgosID)) %>%
  group_by(ID.f) %>%
  mutate(DaysSinceFirst = as.double((LocalDateTime - min(LocalDateTime))/(60*60*24)))

pre.all.01 %>% 
  mutate(DIAG = ifelse(Message_Type == "DIAG", 1, 0),
         DS = ifelse(Message_Type == "DS", 1, 0),
         LC1 = ifelse(LC == "1", 1, 0),
         LC2 = ifelse(LC == "2", 1, 0),
         LC3 = ifelse(LC == "3", 1, 0),
         ARGOS.outside = ifelse(Message_Type == "DIAG" & 
                                  (LC == "1" | LC == "2" | LC == "3") & 
                                  inside == 0, 1, 0),
         GPS = ifelse(Message_Type == "GPS", 1, 0),
         GPS.reject = ifelse(Message_Type == "GPS" & 
                               Residual > 35.0, 1, 0),
         GPS.outside = ifelse(Message_Type == "GPS" 
                              & Residual <= 35.0 & 
                                inside == 0, 1, 0)) %>%
  group_by(ID.f) %>%
  mutate(day1 = ifelse(day1night2 == 1, 1, 0),
         night1 = ifelse(day1night2 == 2, 1, 0), 
         day1a = ifelse(day1night2 == 1 & include == 1, 1, 0),
         night1a= ifelse(day1night2 == 2 & include == 1, 1, 0)) %>%
  summarise(Date1 = min(LocalDateTime), 
            n.days = max(DaysSinceFirst),
            n.relocations.all = n(),
            total.DIAG = sum(DIAG),
            total.DS = sum(DS),
            LC1 = sum(LC1, na.rm = T),
            LC2 = sum(LC2, na.rm = T),
            LC3 = sum(LC3, na.rm = T),
            ARGOS.outside = sum(ARGOS.outside, na.rm = T),
            GPS = sum(GPS, na.rm = T),
            GPS.reject = sum(GPS.reject, na.rm = T),
            GPS.outside = sum(GPS.outside, na.rm = T),
            inside = sum(inside),
            n.day.all = sum(day1),
            n.night.all = sum(night1),
            n.day = sum(day1a),
            n.night = sum(night1a),
            n.relocations = sum(include)) -> pre.summary

if (!file.exists(paste0("data/pre_sample_summary_", 
                        file.date, ".csv")))
  write.csv(pre.summary,
            paste0("data/pre_sample_summary_", 
                   file.date, ".csv"),
            row.names = F, 
            quote = F)

if (!file.exists(paste0(dname, "Pre_GPS_LC_all_", 
                        file.date, ".csv")))
  write.csv(pre.all, paste0(dname, "Pre_GPS_LC_all_", 
                                   file.date, ".csv"),
            row.names = F,
            quote = F)

```

Plot the data points to see what they look like:

```{r}
map.pre <- map.sdbay.zm + 
  geom_polygon(data = SDBay.df,
               aes(x = long, y = lat)) +
               #fill = "blue",alpha = 0.6) + 
  geom_polygon(data = SDBay.eelg.2008.df,
               aes(x = long,
                   y = lat,
                   group = group),
               fill = "green",
               alpha = 0.5) +
  geom_point(data = pre.all,
             aes(x = Lon1, y = Lat1, color = ID.f),
             alpha = 0.5)+ 
  xlab('Longitude') + ylab('Latitude') + 
  theme(legend.title = element_text(size = 10, hjust = 0.5),
        legend.text = element_text(size = 8, vjust = 0),
        legend.position = "none")

plot(map.pre)

```

To check for errors in data screening, I make a few plots. Each individual has one or less data point per 4-hour period. 
```{r}
p1 <- list()
k <- 1
for (k in 1:length(pre.IDs)){
  pre.all.01 %>% filter(ArgosID == pre.IDs[k]) -> dat.01
  pre.all %>% filter(ArgosID == pre.IDs[k]) -> dat.1
  p1[[k]] <-ggplot() + 
    geom_point(data = dat.01,
               aes(x = DaysSinceFirst, 
                   y = hr)) + 
    geom_point(data = dat.1,
               aes(x = DaysSinceFirst,
                   y = hr),
               shape = 1, size = 4,
               color = "red",
               alpha = 0.7) +
    annotate("rect", xmin = 0, xmax = Inf,
             ymin = 4, ymax = 8,
             alpha = 0.3)+
    annotate("rect", xmin = 0, xmax = Inf,
             ymin = 12, ymax = 16,
             alpha = 0.3)+
    annotate("rect", xmin = 0, xmax = Inf,
             ymin = 20, ymax = 24,
             alpha = 0.3) +
    labs(x = "Days since first transmission",
         y = "Hr",
         title = pre.IDs[k])
    # facet_grid(. ~ ID.f) +
    # facet_wrap( ~ ID.f, nrow = 3, 
    #             scales = "free")
    
}

```

Examining the plots by eye, seems to have just one reading per 4-hr period within each day. So, I won't be doing the same for the post dataset. 

Read in the post datasets. 

```{r}
dname <- "data/files_Jul2018_withNewTags/post/"
post.files <- dir(path = dname, 
                 pattern = "_inout_DayNight_4hrs_2018-07-26.csv")

post.IDs <- unlist(lapply(post.files, FUN = ID.names))

post.all <- do.call(rbind, 
                   lapply(post.files,
                          FUN = function(x) readr::read_csv(file = paste0(dname, x),
                                                            col_types = col_def))) %>%
  mutate(ID.f = as.factor(ArgosID)) %>%
  filter(include == 1) %>%
  group_by(ID.f) %>%
  mutate(DaysSinceFirst = as.double((LocalDateTime - min(LocalDateTime))/(60*60*24)))

# This one contains all data:
post.all.01 <- do.call(rbind, 
                       lapply(post.files,
                              FUN = function(x) readr::read_csv(file = paste0(dname, x),
                                                                col_types = col_def))) %>%
  mutate(ID.f = as.factor(ArgosID)) %>%
  group_by(ID.f) %>%
  mutate(DaysSinceFirst = as.double((LocalDateTime - min(LocalDateTime))/(60*60*24)))

map.post <- map.sdbay.zm + 
  geom_polygon(data = SDBay.df,
               aes(x = long, y = lat)) +
  #fill = "blue",
               #alpha = 0.6) + 
  geom_polygon(data = SDBay.eelg.2014.df,
               aes(x = long,
                   y = lat,
                   group = group),
               fill = "green",
               alpha = 0.6) +
  geom_point(data = post.all,
             aes(x = Lon1, y = Lat1, color = ID.f),
             alpha = 0.5)+ 
  labs(color = 'ARGOS ID') + 
  xlab('Longitude') + ylab('Latitude') + 
  theme(#legend.title = element_text(size = 10, hjust = 0.5),
        #legend.text = element_text(size = 8, vjust = 0),
        #legend.position = c(0.2, 0.5),
        legend.position = "none")

plot(map.post)
```

And probably we should inlude the new files into "post". Also, take out the one track that goes out the bay - north of 32.66N
```{r}
dname <- "data/files_Jul2018_withNewTags/new/"
new.files <- dir(path = dname, 
                 pattern = "_inout_DayNight_4hrs_2018-07-26.csv")

new.IDs <- unlist(lapply(new.files, FUN = ID.names))

col_def <- cols(ID = col_integer(),
                ArgosID = col_integer(),
                Message_Type = col_factor(levels = c("DIAG", "DS", "GPS")),
                TransmissionDateTime = col_datetime(format = "%m/%d/%Y %H:%M:%S"),
                LC = col_integer(),
                Residual = col_double(),
                Lat1 = col_double(),
                Lon1 = col_double(),
                inside = col_integer(),
                UTCDateTime = col_datetime(format = "%Y-%m-%d %H:%M:%S"),
                LocalDateTime = col_datetime(format = "%Y-%m-%d %H:%M:%S"),
                Date2 = col_date(format = "%Y/%m/%d"),
                LocalSunrise = col_datetime(format = "%Y-%m-%d %H:%M:%S"),
                LocalSunset =  col_datetime(format = "%Y-%m-%d %H:%M:%S"),
                day1night2 = col_integer(),
                row = col_integer(),
                include = col_integer(),
                hr = col_integer())
new.all <- do.call(rbind, 
                   lapply(new.files,
                          FUN = function(x) readr::read_csv(file = paste0(dname, x),
                                                            col_types = col_def))) %>%
  mutate(ID.f = as.factor(ArgosID)) %>%
  filter(include == 1) %>%
  group_by(ID.f) %>%
  mutate(DaysSinceFirst = as.double((LocalDateTime - min(LocalDateTime))/(60*60*24)))

new.all.01 <- do.call(rbind, 
                      lapply(new.files,
                             FUN = function(x) readr::read_csv(file = paste0(dname, x),
                                                               col_types = col_def))) %>%
  mutate(ID.f = as.factor(ArgosID)) %>%
  group_by(ID.f) %>%
  mutate(DaysSinceFirst = as.double((LocalDateTime - min(LocalDateTime))/(60*60*24)))

post.all <- rbind(post.all, new.all) %>%
  filter(Lat1 < 32.66)

post.all.01 <- rbind(post.all.01, new.all.01) %>%
  filter(Lat1 < 32.66)

post.all.01 %>%   
  mutate(DIAG = ifelse(Message_Type == "DIAG", 1, 0),
         DS = ifelse(Message_Type == "DS", 1, 0),
         LC1 = ifelse(LC == "1", 1, 0),
         LC2 = ifelse(LC == "2", 1, 0),
         LC3 = ifelse(LC == "3", 1, 0),
         ARGOS.outside = ifelse(Message_Type == "DIAG" & 
                                  (LC == "1" | LC == "2" | LC == "3") & 
                                  inside == 0, 1, 0),
         GPS = ifelse(Message_Type == "GPS", 1, 0),
         GPS.reject = ifelse(Message_Type == "GPS" & 
                               Residual > 35.0, 1, 0),
         GPS.outside = ifelse(Message_Type == "GPS" 
                              & Residual <= 35.0 & 
                                inside == 0, 1, 0)) %>%
  group_by(ID.f) %>%
  mutate(day1 = ifelse(day1night2 == 1, 1, 0),
         night1 = ifelse(day1night2 == 2, 1, 0), 
         day1a = ifelse(day1night2 == 1 & include == 1, 1, 0),
         night1a= ifelse(day1night2 == 2 & include == 1, 1, 0)) %>%
  summarise(Date1 = min(LocalDateTime), 
            n.days = max(DaysSinceFirst),
            n.relocations.all = n(),
            total.DIAG = sum(DIAG),
            total.DS = sum(DS),
            LC1 = sum(LC1, na.rm = T),
            LC2 = sum(LC2, na.rm = T),
            LC3 = sum(LC3, na.rm = T),
            LC.reject = sum(LC.other, na.rm = T),
            ARGOS.outside = sum(ARGOS.outside, na.rm = T),
            GPS = sum(GPS, na.rm = T),
            GPS.reject = sum(GPS.reject, na.rm = T),
            GPS.outside = sum(GPS.outside, na.rm = T),
            inside = sum(inside),
            n.day.all = sum(day1),
            n.night.all = sum(night1),
            n.day = sum(day1a),
            n.night = sum(night1a),
            n.relocations = sum(include)) -> post.summary


if (!file.exists(paste0("data/post_sample_summary_", 
                        file.date, ".csv")))
  write.csv(post.summary,
            paste0("data/post_sample_summary_", 
                   file.date, ".csv"),
            quote = F,
            row.names = F)

if (!file.exists(paste0(dname, "Post_GPS_LC_all_", 
                        file.date, ".csv")))
  write.csv(post.all, 
            paste0(dname, "Post_GPS_LC_all_",
                   file.date, ".csv"),
            quote = F,
            row.names = F)

map.post <- map.sdbay.zm + 
  geom_polygon(data = SDBay.df,
               aes(x = long, y = lat)) + 
               #fill = "blue",
               #alpha = 0.6) + 
  geom_polygon(data = SDBay.eelg.2014.df,
               aes(x = long,
                   y = lat,
                   group = group),
               fill = "green",
               alpha = 0.6) +geom_point(data = post.all,
             aes(x = Lon1, y = Lat1, color = ID.f),
             alpha = 0.5)+ 
  labs(color = 'ARGOS ID') + 
  xlab('Longitude') + ylab('Latitude') + 
  theme(#legend.title = element_text(size = 10, hjust = 0.5),
        #legend.text = element_text(size = 8, vjust = 0),
        #legend.position = c(0.2, 0.5),
    legend.position = "none")

plot(map.post)
```

We have a lot more individuals from the post period than the pre, so we need to do something about that... especially for computing HRs. 


Section 3: Compute HRs

```{r}
# when trying to compute HR for each individual, some don't have enough data
# extract those with at least 50 data points:

pre.kd <- HR.analysis.step1(min_n, pre.all, tagproj, grid = 300)
```

Do the same with post dataset - choose those with at least min_n data points. 

```{r}
post.kd <- HR.analysis.step1(min_n, post.all, tagproj, grid = 300)

```

Then the home range analysis starts here. First we need to figure out the smoothing parameter (bandwidth, h). There seem to be different methods and recommendations...

First, least-squares cross validation on the pre data:
```{r}
plotLSCV(pre.kd$kd.LSCV)

```

Referecne or optimum bandwidth is ~248 (see pre.kd.href), whereas the LSCV comes back with h = 25.9 (Pre.kd@h$h). According to Kie (2013), we should decrease href in increments of 0.10 and the best value is the smallest increment of href that 1. resulted in a contiguous rather than disjoint 95% kernel home-range polygon, and 2. contained no lacuna within the home range. 
```{r}
h.multiplier <-  seq(from = 0.1, to = 0.95, by = 0.05) 

h.adhoc.pre <- HR.analysis(h.multiplier, 
                           pre.kd$kd.href, 
                           pre.kd$list.data, 
                           grid = 300)
  
plot(h.adhoc.pre$figure)


```

Looking at these plots, multiplier = 0.6 may be the smallest value with a contiguous 95% UD.  This is equivalent of h = 148.96.  Rounded to 149. 

```{r}

h.pre.1 <- find.h.adhoc(pre.kd$list.data$all.utm)
## Section 3.1.2
##Visually optimized hlim = c(0.565,1.5), grid=300
# grid values from 50 to 300 don't change the outcome;
# extent values from 1 to 200 don't change the outcome either

h.pre <- round(h.pre.1$h)

pre.HR <- compute.area(h.pre, 
                       pre.kd$list.data, 
                       grid = 300)

writeOGR(obj = pre.HR$ver.50, 
         dsn = "shapefiles_July2018/pre_HR_50.shp",
         layer="Pre.50", 
         driver="ESRI Shapefile", 
         overwrite_layer=TRUE)

writeOGR(obj = pre.HR$ver.95, 
         dsn = "shapefiles_July2018/pre_HR_95.shp",
         layer="Pre.95", 
         driver="ESRI Shapefile", 
         overwrite_layer=TRUE)
# plot(pre.ver.50, axes = TRUE)
# plot(pre.ver.95, axes = TRUE)
#points(pre.all.utm, col="blue")

```


Do the same for the post data - here we need to look at subsampling individuals. First, use all data.

First, least-squares cross validation on the pre data:
```{r}
plotLSCV(post.kd$kd.LSCV)

```

```{r}

h.adhoc.post <- HR.analysis(h.multiplier, 
                            post.kd$kd.href, 
                            post.kd$list.data, 
                            grid = 300)
  
plot(h.adhoc.post$figure)

```

For post data, multiplier = 0.4 may be the smallest value with a contiguous 95% UD.  This is equivalent of h = 97.16. 

```{r}
## Section 3.1.2
##Visually optimized hlim = c(0.565,1.5), grid=300
h.post.1 <- find.h.adhoc(post.kd$list.data$all.utm)

h.post <- round(h.post.1$h) #round(post.kd$kd.href@h$h * 0.4)
post.HR <- compute.area(h.post, post.kd$list.data, grid = 300)
writeOGR(obj = post.HR$ver.50, 
         dsn = "shapefiles_July2018/post_HR_50.shp",
         layer="Post.50", 
         driver="ESRI Shapefile", 
         overwrite_layer=TRUE)

writeOGR(obj = post.HR$ver.95, 
         dsn = "shapefiles_July2018/post_HR_95.shp",
         layer="Post.95", 
         driver="ESRI Shapefile", 
         overwrite_layer=TRUE)

# plot(post.ver.50, axes = TRUE)
# plot(post.ver.95, axes = TRUE)
#points(pre.all.utm, col="blue")
```

Select all possible combinations of 6 (ID.pre.min_n) individuals from the post data. There are 8008 possible combinations when min_n = 50. It takes a bit of time, I select 1000 of them. Because the sample size decreases, I use h.pre for this.

```{r echo=FALSE, results="hide"}
run.date <- "2018-07-17"
if (!file.exists(paste0("RData/areas_combos_", run.date, ".rds"))){
  combos <- combn(1:dim(ID.post.min_n)[1], 
                  dim(ID.pre.min_n)[1])
  combos <- combos[, sample(1:ncol(combos), 
                            size = 1000)]
  IDs <- ver.95.df.list <- pts.df.list <- vector(mode = "list", length = ncol(combos))
  area.95 <- area.50 <- vector(mode = "numeric", length = ncol(combos))
  
  #p.list <- list()
  for (k in 1:ncol(combos)){
    
    ID.post.tmp <- ID.post.min_n[combos[,k], ]
    tmp.list <- make.HR.dataset(post.all, ID.post.tmp, tagproj)
    
    tmp.HR <- HR.bvnorm.fcn(all.utm = tmp.list$all.utm,
                            byID.utm = tmp.list$byID.utm,
                            h = h.pre,
                            hlim=c(0.03, 1.5),
                            grid=300, extent = 1)
    
    ver.95.df.list <- broom::tidy(spTransform(getverticeshr(tmp.HR$all.kd, 95),
                                              CRS("+proj=longlat")))
    
    pts.df.list <- data.frame(lon = tmp.list$all.coords@coords[,1],
                              lat = tmp.list$all.coords@coords[,2])
    
    # p.list[[k]] <- ggplot() +
    #   geom_polygon(data = tmp.df, aes(x = long, y = lat, group = group)) +
    #   geom_point(data = pts.df, aes(x = lon, y = lat),
    #              color = "green") + coord_map()
    
    area.50[k] <- tmp.HR$area.all["50"]
    area.95[k] <- tmp.HR$area.all["95"]
    
    IDs[[k]] <- ID.post.tmp[,1]
  }
  
  areas.combos <- list(area95 = area.95,
                       area50 = area.50,
                       ID = IDs,
                       polygon = ver.95.df.list,
                       points = pts.df.list)
  
  saveRDS(areas.combos,
          file = paste0("RData/areas_combos_", Sys.Date(), ".rds"))
  
} else {
  areas.combos <- readRDS(file = paste0("RData/areas_combos_", 
                                        run.date, ".rds"))
}

```

Compare the randomized areas and the pre area.
```{r}
dif.50.df <- data.frame(dif = areas.combos$area50 - pre.HR$area.50)
dif.95.df <- data.frame(dif = areas.combos$area95 - pre.HR$area.95)

hist.dif.50 <- ggplot(data = dif.50.df) + 
  geom_histogram(aes(x = dif), binwidth = 0.1) +
  labs(x = "Difference in area", y = "Count")

#plot(hist.dif.50)

hist.dif.95 <- ggplot(data = dif.95.df) + 
  geom_histogram(aes(x = dif), binwidth = 0.15) +
  labs(x = "Difference in area", y = "Count")

#plot(hist.dif.95)

# quantile(dif.50.df$dif, c(0.025, 0.05, 0.10, 0.15, 0.50, 0.975))
# quantile(dif.95.df$dif, c(0.025, 0.05, 0.10, 0.15, 0.50, 0.975))
# 
# dif.50.df %>% count(dif>0)
# 
# dif.95.df %>% count(dif > 0)


```


The 95 percentiles of UDs ranged from 5.95 km2 to 7.89 when using six turtles at a time. The computed 95% UD for the pre dataset was 6.31, which was about 10 percentile of UDs for the post dataset. This indicated the UDs increased from the pre to post periods. 

Compare between night and day and pre and post:
```{r}
# pre day vs pre night:
pre.all %>% filter(day1night2 == 1) -> pre.day

pre.kd.day <- HR.analysis.step1(min_n, pre.day, tagproj, grid = 300)

plotLSCV(pre.kd.day$kd.LSCV)
```

```{r}
h.adhoc.pre.day <- HR.analysis(h.multiplier, 
                               pre.kd.day$kd.href, 
                               pre.kd.day$list.data, 
                               grid = 300)
plot(h.adhoc.pre.day$figure)

```

multiplier = 0.7 may be good. 

```{r}
h.pre.day.1 <- find.h.adhoc(pre.kd.day$list.data$all.utm)

h.pre.day <- round(h.pre.day.1$h) #round(pre.kd.day$kd.href@h$h * 0.7)
# or use the same value as before

pre.HR.day <- compute.area(h.pre.day, pre.kd.day$list.data, grid = 300)
writeOGR(obj = pre.HR.day$ver.50, 
         dsn = "shapefiles_July2018/pre_HR_day_50.shp",
         layer="Pre.day.50", 
         driver="ESRI Shapefile", 
         overwrite_layer=TRUE)

writeOGR(obj = pre.HR.day$ver.95, 
         dsn = "shapefiles_July2018/pre_HR_day_95.shp",
         layer="Pre.day.95", 
         driver="ESRI Shapefile", 
         overwrite_layer=TRUE)
```

For night:
```{r}
pre.all %>% filter(day1night2 == 2) -> pre.night
pre.kd.night <- HR.analysis.step1(min_n, pre.night, tagproj, grid = 300)
plotLSCV(pre.kd.night$kd.LSCV)

```

```{r}
h.adhoc.pre.night <- HR.analysis(h.multiplier, 
                                 pre.kd.night$kd.href, 
                                 pre.kd.night$list.data, 
                                 grid = 300)
plot(h.adhoc.pre.night$figure)

```

multiplier = 0.5 may be good. 

```{r}
h.pre.night.1 <- find.h.adhoc(pre.kd.night$list.data$all.utm)

h.pre.night <- round(h.pre.night.1$h) #round(pre.kd.night$kd.href@h$h * 0.5)
# or use the same value as before

pre.HR.night <- compute.area(h.pre.night, 
                             pre.kd.night$list.data, 
                             grid = 300)

writeOGR(obj = pre.HR.night$ver.50, 
         dsn = "shapefiles_July2018/pre_HR_night_50.shp",
         layer="Pre.night.50", 
         driver="ESRI Shapefile", 
         overwrite_layer=TRUE)

writeOGR(obj = pre.HR.night$ver.95, 
         dsn = "shapefiles_July2018/pre_HR_night_95.shp",
         layer="Pre.night.95", 
         driver="ESRI Shapefile", 
         overwrite_layer=TRUE)
```

Do the same with the post period
```{r}
# pre day vs pre night:
post.all %>% filter(day1night2 == 1) -> post.day

post.kd.day <- HR.analysis.step1(min_n, post.day, tagproj, grid = 300)

plotLSCV(post.kd.day$kd.LSCV)
```

```{r}
h.adhoc.post.day <- HR.analysis(h.multiplier, 
                                post.kd.day$kd.href, 
                                post.kd.day$list.data, 
                                grid = 300)
plot(h.adhoc.post.day$figure)

```

multiplier = 0.4 may be good. 

```{r}
h.post.day <- round(post.kd.day$kd.href@h$h * 0.4)
# or use the same value as before

post.HR.day <- compute.area(h.post.day, post.kd.day$list.data, grid = 300)
writeOGR(obj = post.HR.day$ver.50, 
         dsn = "shapefiles_July2018/post_HR_day_50.shp",
         layer="Post.day.50", 
         driver="ESRI Shapefile", 
         overwrite_layer=TRUE)

writeOGR(obj = post.HR.day$ver.95, 
         dsn = "shapefiles_July2018/post_HR_day_95.shp",
         layer="Post.day.95", 
         driver="ESRI Shapefile", 
         overwrite_layer=TRUE)

```

For night:
```{r}
post.all %>% filter(day1night2 == 2) -> post.night
post.kd.night <- HR.analysis.step1(min_n, post.night, tagproj, grid = 300)
plotLSCV(post.kd.night$kd.LSCV)

```

```{r}
h.adhoc.post.night <- HR.analysis(h.multiplier, 
                                  post.kd.night$kd.href, 
                                  post.kd.night$list.data, 
                                  grid = 300)
plot(h.adhoc.post.night$figure)

```

multiplier = 0.45 may be good. 

```{r}
h.post.night <- round(post.kd.night$kd.href@h$h * 0.45)
# or use the same value as before

post.HR.night <- compute.area(h.post.night, post.kd.night$list.data, grid = 300)

writeOGR(obj = post.HR.night$ver.50, 
         dsn = "shapefiles_July2018/post_HR_night_50.shp",
         layer="Post.night.50", 
         driver="ESRI Shapefile", 
         overwrite_layer=TRUE)

writeOGR(obj = post.HR.night$ver.95, 
         dsn = "shapefiles_July2018/post_HR_night_95.shp",
         layer="Post.night.95", 
         driver="ESRI Shapefile", 
         overwrite_layer=TRUE)

```

Compute UDs for each individual. Starting with pre:
```{r}
h.pre.eachID <- h.multip.pre.eachID <- vector()
UD.95.pre.eachID <- UD.50.pre.eachID <- list()
# first find appropriate bandwidth for each individual:
for (k in 1:length(pre.kd$list.data$eachID.utm)){
  dat.utm <- pre.kd$list.data$eachID.utm[[k]]
  best.h <- find.h.adhoc(dat.utm)
  h.pre.eachID[k] <- round(best.h$h)
  h.multip.pre.eachID[k] <- best.h$h.multip
  UD <- kernelUD(dat.utm, h = h.pre.eachID[k], 
                 kern = "bivnorm", grid = 300)
  UD.95.pre.eachID[[k]] <- getverticeshr(UD, 95)
  UD.50.pre.eachID[[k]] <- getverticeshr(UD, 50)
}

tmp.pre.95 <- lapply(UD.95.pre.eachID, 
                 FUN = function(x) broom::tidy(spTransform(x, CRS("+proj=longlat"))))
tmp.pre.50 <- lapply(UD.50.pre.eachID, 
                 FUN = function(x) broom::tidy(spTransform(x, CRS("+proj=longlat"))))

for (k in 1:length(tmp.pre.95)){
  tmp.pre.50[[k]] <- mutate(tmp.pre.50[[k]], ID = pre.kd$list.data$unique.ID[k])
  tmp.pre.95[[k]] <- mutate(tmp.pre.95[[k]], ID = pre.kd$list.data$unique.ID[k])
}

pre.eachID.ver.95.df <- do.call("rbind", tmp.pre.95)
pre.eachID.ver.50.df <- do.call("rbind", tmp.pre.50)
```

Now for post:
```{r}
h.post.eachID <- h.multip.post.eachID <- vector()
UD.95.post.eachID <- UD.50.post.eachID <- list()
# first find appropriate bandwidth for each individual:
for (k in 1:length(post.kd$list.data$eachID.utm)){
  dat.utm <- post.kd$list.data$eachID.utm[[k]]
  best.h <- find.h.adhoc(dat.utm)
  h.post.eachID[k] <- round(best.h$h)
  h.multip.post.eachID[k] <- best.h$h.multip
  UD <- kernelUD(dat.utm, h = h.post.eachID[k], 
                 kern = "bivnorm", 
                 grid = 300)
  UD.95.post.eachID[[k]] <- getverticeshr(UD, 95)
  UD.50.post.eachID[[k]] <- getverticeshr(UD, 50)
}


tmp.post.95 <- lapply(UD.95.post.eachID, 
                 FUN = function(x) broom::tidy(spTransform(x, CRS("+proj=longlat"))))
tmp.post.50 <- lapply(UD.50.post.eachID, 
                 FUN = function(x) broom::tidy(spTransform(x, CRS("+proj=longlat"))))

for (k in 1:length(tmp.post.95)){
  tmp.post.50[[k]] <- mutate(tmp.post.50[[k]], 
                             ID = post.kd$list.data$unique.ID[k])
  tmp.post.95[[k]] <- mutate(tmp.post.95[[k]], 
                             ID = post.kd$list.data$unique.ID[k])
}

post.eachID.ver.95.df <- do.call("rbind", tmp.post.95) 
post.eachID.ver.50.df <- do.call("rbind", tmp.post.50)



```

Bring in the body size information. This was done in deployment_table.R script and manually modified for missing data - some measurements were not collected when the tags were deployed so data from the closest captures were used.  Data were entered manually for three turtles in the Excel file: 52675 (all measurements) from 2007-01-23, 44366 (SCL and CCL) from 2009-02-26, and 126065 (Mass) from 2011-03-08).   
```{r}
dat.table <- read.csv("data/all_sample_summary_size_2018-07-24.csv")

pre.areas.95 <- unlist(lapply(lapply(UD.95.pre.eachID, 
                                      FUN = extract.areas),
                               FUN = function(x) x$total.area))
pre.areas.50 <- unlist(lapply(lapply(UD.50.pre.eachID, 
                                      FUN = extract.areas),
                               FUN = function(x) x$total.area))

pre.areas.df <- data.frame(ID.f = pre.kd$list.data$unique.ID,
                           area.50 = pre.areas.50/1000000,
                           area.95 = pre.areas.95/1000000)

post.areas.95 <- unlist(lapply(lapply(UD.95.post.eachID, 
                                      FUN = extract.areas),
                               FUN = function(x) x$total.area))
post.areas.50 <- unlist(lapply(lapply(UD.50.post.eachID, 
                                      FUN = extract.areas),
                               FUN = function(x) x$total.area))
post.areas.df <- data.frame(ID.f = post.kd$list.data$unique.ID,
                           area.50 = post.areas.50/1000000,
                           area.95 = post.areas.95/1000000)

areas.df <- rbind(pre.areas.df, post.areas.df)

dat.table <- left_join(dat.table, areas.df, by = "ID.f")

ggplot(data = dat.table) +
  geom_point(aes(x = Mass_kg, 
                 y = area.50/1000000,
                 color = period)) 
```




Plotting starts here:
Make a plot for pre using all data:
```{r}

pre.ver.50.df <- broom::tidy(spTransform(pre.HR$ver.50, CRS("+proj=longlat")))
pre.ver.95.df <- broom::tidy(spTransform(pre.HR$ver.95, CRS("+proj=longlat")))

map.pre.HR <- map.sdbay.zm + 
  geom_polygon(data = SDBay.df,
               aes(x = long, y = lat)) +
               #fill = "blue",alpha = 0.6) + 
  geom_polygon(data = SDBay.eelg.2008.df,
               aes(x = long,
                   y = lat,
                   group = group),
               fill = "green",
               alpha = 0.5) +
  geom_polygon(data = pre.ver.95.df,
               aes(x = long, y = lat,
                   group = group),
               fill = "cornsilk",
               alpha = 0.6) + 
  geom_polygon(data = pre.ver.50.df,
               aes(x = long, y = lat,
                   group = group),
               fill = "red3",
               alpha = 0.6) + 
  # geom_point(data = pre.all,
  #            aes(x = Lon1, y = Lat1),
  #            alpha = 0.5,
  #            size = 0.2)+ 
  # geom_polygon(data = water.df,
  #              aes(x = long,
  #                  y = lat,
  #                  group = group),
  #              fill = "blue",
  #              alpha = 0.2) +

  #coord_map() + 
  #labs(color = 'ARGOS ID') + 
  xlab('Longitude') + ylab('Latitude') + 
  theme(#legend.title = element_text(size = 10, hjust = 0.5),
        #legend.text = element_text(size = 8, vjust = 0),
        legend.position = "none")


#plot(map.pre.HR)
```

Make a plot for pre day:
```{r}

pre.day.ver.50.df <- broom::tidy(spTransform(pre.HR.day$ver.50, CRS("+proj=longlat")))
pre.day.ver.95.df <- broom::tidy(spTransform(pre.HR.day$ver.95, CRS("+proj=longlat")))

map.pre.day.HR <- map.sdbay.zm + 
  geom_polygon(data = SDBay.df,
               aes(x = long, y = lat)) +
               #fill = "blue",alpha = 0.6) + 
  geom_polygon(data = SDBay.eelg.2008.df,
               aes(x = long,
                   y = lat,
                   group = group),
               fill = "green",
               alpha = 0.5) +
  geom_polygon(data = pre.day.ver.95.df,
               aes(x = long, y = lat,
                   group = group),
               fill = "cornsilk",
               alpha = 0.6) + 
  geom_polygon(data = pre.day.ver.50.df,
               aes(x = long, y = lat,
                   group = group),
               fill = "red3",
               alpha = 0.6) + 
  # geom_point(data = pre.all,
  #            aes(x = Lon1, y = Lat1),
  #            alpha = 0.5,
  #            size = 0.2)+ 
  # geom_polygon(data = water.df,
  #              aes(x = long,
  #                  y = lat,
  #                  group = group),
  #              fill = "blue",
  #              alpha = 0.2) +

  #coord_map() + 
  #labs(color = 'ARGOS ID') + 
  xlab('Longitude') + ylab('Latitude') + 
  theme(#legend.title = element_text(size = 10, hjust = 0.5),
        #legend.text = element_text(size = 8, vjust = 0),
        legend.position = "none")


#plot(map.pre.day.HR)
```

Make a plot for pre night:
```{r}

pre.night.ver.50.df <- broom::tidy(spTransform(pre.HR.night$ver.50, CRS("+proj=longlat")))
pre.night.ver.95.df <- broom::tidy(spTransform(pre.HR.night$ver.95, CRS("+proj=longlat")))

map.pre.night.HR <- map.sdbay.zm + 
  geom_polygon(data = SDBay.df,
               aes(x = long, y = lat)) +
               #fill = "blue",alpha = 0.6) + 
  geom_polygon(data = SDBay.eelg.2008.df,
               aes(x = long,
                   y = lat,
                   group = group),
               fill = "green",
               alpha = 0.5) +
  geom_polygon(data = pre.night.ver.95.df,
               aes(x = long, y = lat,
                   group = group),
               fill = "cornsilk",
               alpha = 0.6) + 
  geom_polygon(data = pre.night.ver.50.df,
               aes(x = long, y = lat,
                   group = group),
               fill = "red3",
               alpha = 0.6) + 
  # geom_point(data = pre.all,
  #            aes(x = Lon1, y = Lat1),
  #            alpha = 0.5,
  #            size = 0.2)+ 
  # geom_polygon(data = water.df,
  #              aes(x = long,
  #                  y = lat,
  #                  group = group),
  #              fill = "blue",
  #              alpha = 0.2) +

  #coord_map() + 
  #labs(color = 'ARGOS ID') + 
  xlab('Longitude') + ylab('Latitude') + 
  theme(#legend.title = element_text(size = 10, hjust = 0.5),
        #legend.text = element_text(size = 8, vjust = 0),
        legend.position = "none")


#plot(map.pre.night.HR)
```

Make a plot for post using all data:
```{r}

post.ver.50.df <- broom::tidy(spTransform(post.HR$ver.50, CRS("+proj=longlat")))
post.ver.95.df <- broom::tidy(spTransform(post.HR$ver.95, CRS("+proj=longlat")))

map.post.HR <- map.sdbay.zm + 
  geom_polygon(data = SDBay.df,
               aes(x = long, y = lat)) +
               #fill = "blue",alpha = 0.6) + 
  geom_polygon(data = SDBay.eelg.2014.df,
               aes(x = long,
                   y = lat,
                   group = group),
               fill = "green",
               alpha = 0.5) +
  geom_polygon(data = post.ver.95.df,
               aes(x = long, y = lat,
                   group = group),
               fill = "cornsilk",
               alpha = 0.6) + 
  geom_polygon(data = post.ver.50.df,
               aes(x = long, y = lat, 
                   group = group),
               fill = "red3",
               alpha = 0.6) + 
  # geom_point(data = post.all,
  #            aes(x = Lon1, y = Lat1),
  #            alpha = 0.5,
  #            size = 0.2)+ 
  # # geom_polygon(data = water.df,
  #              aes(x = long,
  #                  y = lat,
  #                  group = group),
  #              fill = "blue",
  #              alpha = 0.2) +

  #coord_map() + 
  #labs(color = 'ARGOS ID') + 
  xlab('Longitude') + ylab('Latitude') + 
  theme(#legend.title = element_text(size = 10, hjust = 0.5),
        #legend.text = element_text(size = 8, vjust = 0),
        legend.position = "none")
#plot(map.post.HR)
```

Make a plot for post day:
```{r}

post.day.ver.50.df <- broom::tidy(spTransform(post.HR.day$ver.50, CRS("+proj=longlat")))
post.day.ver.95.df <- broom::tidy(spTransform(post.HR.day$ver.95, CRS("+proj=longlat")))

map.post.day.HR <- map.sdbay.zm + 
  geom_polygon(data = SDBay.df,
               aes(x = long, y = lat)) +
               #fill = "blue",alpha = 0.6) + 
  geom_polygon(data = SDBay.eelg.2014.df,
               aes(x = long,
                   y = lat,
                   group = group),
               fill = "green",
               alpha = 0.5) +
  geom_polygon(data = post.day.ver.95.df,
               aes(x = long, y = lat,
                   group = group),
               fill = "cornsilk",
               alpha = 0.6) + 
  geom_polygon(data = post.day.ver.50.df,
               aes(x = long, y = lat, 
                   group = group),
               fill = "red3",
               alpha = 0.6) + 
  # geom_point(data = post.all,
  #            aes(x = Lon1, y = Lat1),
  #            alpha = 0.5,
  #            size = 0.2)+ 
  # # geom_polygon(data = water.df,
  #              aes(x = long,
  #                  y = lat,
  #                  group = group),
  #              fill = "blue",
  #              alpha = 0.2) +

  #coord_map() + 
  #labs(color = 'ARGOS ID') + 
  xlab('Longitude') + ylab('Latitude') + 
  theme(#legend.title = element_text(size = 10, hjust = 0.5),
        #legend.text = element_text(size = 8, vjust = 0),
        legend.position = "none")

#plot(map.post.day.HR)
```


Make a plot for post night:
```{r}

post.night.ver.50.df <- broom::tidy(spTransform(post.HR.night$ver.50,
                                                CRS("+proj=longlat")))
post.night.ver.95.df <- broom::tidy(spTransform(post.HR.night$ver.95,
                                                CRS("+proj=longlat")))

map.post.night.HR <- map.sdbay.zm + 
  geom_polygon(data = SDBay.df,
               aes(x = long, y = lat)) +
               #fill = "blue",alpha = 0.6) + 
  geom_polygon(data = SDBay.eelg.2014.df,
               aes(x = long,
                   y = lat,
                   group = group),
               fill = "green",
               alpha = 0.5) +
  geom_polygon(data = post.night.ver.95.df,
               aes(x = long, y = lat,
                   group = group),
               fill = "cornsilk",
               alpha = 0.6) + 
  geom_polygon(data = post.night.ver.50.df,
               aes(x = long, y = lat, 
                   group = group),
               fill = "red3",
               alpha = 0.6) + 
  xlab('Longitude') + ylab('Latitude') + 
  theme(#legend.title = element_text(size = 10, hjust = 0.5),
        #legend.text = element_text(size = 8, vjust = 0),
        legend.position = "none")

#plot(map.post.night.HR)
```

Simple plots with day/night for the pre:
Plot the data points to see what they look like:

```{r}
map.pre.day.night <- map.sdbay.zm + 
  geom_polygon(data = SDBay.df,
               aes(x = long, y = lat)) +
               #fill = "blue",alpha = 0.6) + 
  geom_polygon(data = SDBay.eelg.2008.df,
               aes(x = long,
                   y = lat,
                   group = group),
               fill = "green",
               alpha = 0.5) +
  geom_point(data = pre.all,
             aes(x = Lon1, y = Lat1, 
                 color = as.factor(day1night2)),
             alpha = 0.5)+ 
  xlab('Longitude') + ylab('Latitude') + 
  scale_color_discrete(labels = c("Day", "Night"),
                       name = NULL) 

  theme(legend.title = NULL,
        legend.text = element_text(size = 8, vjust = 0))
        #legend.position = "none")

plot(map.pre.day.night)

```

```{r}
map.post.day.night <- map.sdbay.zm + 
  geom_polygon(data = SDBay.df,
               aes(x = long, y = lat)) +
               #fill = "blue",alpha = 0.6) + 
  geom_polygon(data = SDBay.eelg.2008.df,
               aes(x = long,
                   y = lat,
                   group = group),
               fill = "green",
               alpha = 0.5) +
  geom_point(data = post.all,
             aes(x = Lon1, y = Lat1, 
                 color = as.factor(day1night2)),
             alpha = 0.5)+ 
  xlab('Longitude') + ylab('Latitude') + 
  scale_color_discrete(labels = c("Day", "Night"),
                       name = NULL) 

plot(map.post.day.night)

```


UDs for individual turtles:
```{r}

p.h.pre.eachID <- ggplot() + 
  geom_polygon(data = pre.eachID.ver.95.df,
               aes(x = long, y = lat,
                   group = group),
               fill = "aquamarine4",
               alpha = 0.6) + 
  geom_polygon(data = pre.eachID.ver.50.df,
               aes(x = long, y = lat,
                   group = group),
               fill = "red3",
               alpha = 0.6) +
  coord_map() + 
  facet_grid(. ~ ID) +
  facet_wrap( ~ ID, nrow = 2) + 
  labs(x = "", y = "")+ 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5)) +
  scale_x_continuous(breaks = c(-117.14, -117.12, -117.10),
                     limits = c(-117.15, -117.09)) +
  scale_y_continuous(breaks = c(32.60, 32.62, 32.64),
                     limits = c(32.59, 32.65))
plot(p.h.pre.eachID)
```

```{r}

p.h.post.eachID <- ggplot() + 
  geom_polygon(data = post.eachID.ver.95.df,
               aes(x = long, y = lat,
                   group = group),
               fill = "aquamarine4",
               alpha = 0.6) + 
  geom_polygon(data = post.eachID.ver.50.df,
               aes(x = long, y = lat,
                   group = group),
               fill = "red3",
               alpha = 0.6) + 
  coord_map() + facet_grid(. ~ ID) +
  facet_wrap( ~ ID, nrow = 3) + 
  labs(x = "", y = "")+ 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5)) +
  scale_x_continuous(breaks = c(-117.14, -117.12, -117.10, -117.08),
                     limits = c(-117.15, -117.07)) +
  scale_y_continuous(breaks = c(32.60, 32.62, 32.64),
                     limits = c(32.59, 32.66))
plot(p.h.post.eachID)
```



Save figures:
```{r}
if (save.figure){
  ggsave(filename = paste0("figures/HR_pre_h_comp_", Sys.Date(), ".png"),
       plot = h.adhoc.pre$figure,
       dpi = 600,
       device = "png")
  ggsave(filename = paste0("figures/HR_post_h_comp_", Sys.Date(), ".png"),
         plot = h.adhoc.post$figure,
         dpi = 600,
         device = "png")
  ggsave(filename = paste0("figures/HR_pre_day_h_comp_", Sys.Date(), ".png"),
         plot = h.adhoc.pre.day$figure,
         dpi = 600,
         device = "png")
  ggsave(filename = paste0("figures/HR_pre_night_h_comp_", Sys.Date(), ".png"),
         plot = h.adhoc.pre.night$figure,
         dpi = 600,
         device = "png")
  ggsave(filename = paste0("figures/HR_post_day_h_comp_", Sys.Date(), ".png"),
         plot = h.adhoc.post.day$figure,
         dpi = 600,
         device = "png")
  ggsave(filename = paste0("figures/HR_post_night_h_comp_", Sys.Date(), ".png"),
         plot = h.adhoc.post.night$figure,
         dpi = 600,
         device = "png")
  ggsave(filename = paste0("figures/HR_pre_h_", h.pre, "_", Sys.Date(), ".png"),
         plot = map.pre.HR,
         dpi = 600,
         device = "png")
  
  ggsave(filename = paste0("figures/HR_post_h_", h.post, "_", Sys.Date(), ".png"),
         plot = map.post.HR,
         dpi = 600,
         device = "png")
  
  ggsave(filename = paste0("figures/HR_pre_day_h_", 
                           h.pre.day, "_", Sys.Date(), ".png"),
         plot = map.pre.day.HR,
         dpi = 600,
         device = "png")
  
  ggsave(filename = paste0("figures/HR_post_day_h_", 
                           h.post.day, "_", Sys.Date(), ".png"),
         plot = map.post.day.HR,
         dpi = 600,
         device = "png")
  
  ggsave(filename = paste0("figures/HR_pre_night_h_", 
                           h.pre.night, "_", Sys.Date(), ".png"),
         plot = map.pre.night.HR,
         dpi = 600,
         device = "png")
  
  ggsave(filename = paste0("figures/HR_post_night_h_", 
                           h.post.night, "_", Sys.Date(), ".png"),
         plot = map.post.night.HR,
         dpi = 600,
         device = "png")
  
  ggsave(filename = paste0("figures/HR95_difference_", 
                           Sys.Date(), ".png"),
         plot = hist.dif.95,
         dpi = 600,
         device = "png")

  ggsave(filename = paste0("figures/HR50_difference_", 
                           Sys.Date(), ".png"),
         plot = hist.dif.50,
         dpi = 600,
         device = "png")
  
  ggsave(filename = paste0("figures/HRbyID_pre_",
                           Sys.Date(), ".png"),
         plot = p.h.pre.eachID,
         dpi = 600,
         device = "png")
  ggsave(filename = paste0("figures/HRbyID_post_",
                           Sys.Date(), ".png"),
         plot = p.h.post.eachID,
         dpi = 600,
         device = "png")
}


#plot(map.post.HR)
```